# Research: Single-Address Geocoding Endpoint

**Feature**: 003-address-geocoding-endpoint
**Date**: 2026-02-13

## Research Topics

### 1. Additional Datastore (Redis / OpenSearch / Memcache)

**Decision**: Use PostgreSQL only. Do not add Redis, OpenSearch, or Memcache.

**Rationale**:

The existing PostgreSQL geocoder cache (`geocoder_cache` table) has a unique index on `(provider, normalized_address)` that delivers 0.5-2ms lookup latency. This is 250x faster than the 500ms SLA for cached address returns (SC-002). Adding an in-memory datastore would:

- Add operational complexity (another service to deploy on the single-server piku deployment)
- Require data synchronization between PostgreSQL and the cache layer
- Save only ~1ms per cache hit — imperceptible at 200 req/min rate limit

For the prefix-search autocomplete use case, a PostgreSQL `text_pattern_ops` B-tree index provides sub-millisecond prefix matching for the projected dataset size (low millions of unique addresses). This is simpler and sufficient compared to Redis sorted sets or OpenSearch.

**Alternatives Considered**:

| Option | Latency Gain | Operational Cost | Verdict |
|--------|-------------|-----------------|---------|
| Redis as L1 geocode cache | ~1ms saved per hit | High (new service, connection management, data sync) | Not now. Revisit at multi-instance scale or >1000 req/min. |
| Redis sorted sets for autocomplete | ~0.5ms saved per search | High (separate data structure to maintain) | Not now. PostgreSQL `text_pattern_ops` is sufficient. |
| OpenSearch for autocomplete | Sub-10ms fuzzy search | Very high (cluster, indexing pipeline, schema management) | Not now. Revisit if fuzzy/typo-tolerant matching required. |
| Memcache | ~1ms saved per hit | Medium (simpler than Redis, but no persistence) | Not now. Same reasoning as Redis. |

**When to revisit**: Multi-instance deployment, connection pool contention under load, or requirement for fuzzy/typo-tolerant address matching.

### 2. Freeform Address Normalization Strategy

**Decision**: Extend `lib/geocoder/address.py` with `normalize_freeform_address()`.

**Context**: The existing `reconstruct_address()` function works with decomposed voter address components (street_number, street_name, city, etc.). The new geocode endpoint accepts a single freeform string from the consumer.

**Approach**:

1. Uppercase the entire string
2. Trim leading/trailing whitespace
3. Collapse multiple spaces to single space
4. Apply USPS Pub 28 abbreviations to street types and directionals found within the string (word-boundary matching to avoid false positives)

The resulting string is used as the cache key, ensuring compatibility with keys generated by `reconstruct_address()` for voter addresses.

**Rationale**: A lightweight normalization that maximizes cache hit rates without requiring full address parsing. Full parsing is handled separately in the verification endpoint.

### 3. Address Validation/Parsing for Verification Endpoint

**Decision**: Implement regex-based address component parsing in `lib/geocoder/verify.py`.

**Context**: The verification endpoint (FR-012) must determine whether an address is well-formed by checking for required components: street number, street name, city, state, ZIP.

**Approach**: A regex-based parser that attempts to extract components from a freeform address string. Georgia addresses follow a predictable pattern:

```
[number] [pre-dir] [street name] [street type] [post-dir] [unit], [city], [state] [zip]
```

The parser will:
1. Split on commas to separate street line, city, state/zip
2. Extract ZIP code (5-digit or ZIP+4 pattern)
3. Extract state abbreviation (2 chars preceding ZIP or after city)
4. Extract street number (leading digits)
5. Extract street type (known USPS abbreviations)
6. Remaining tokens form the street name

**Limitations**: Regex-based parsing will not handle all edge cases (e.g., "One Main Street" where the number is spelled out, or PO Boxes). This is acceptable for initial implementation — the spec calls for local validation, not authoritative USPS address verification.

### 4. Prefix Search Performance on PostgreSQL

**Decision**: Use `text_pattern_ops` B-tree index for prefix matching on the `addresses` table.

**Benchmarks** (approximate, based on PostgreSQL documentation and community reports):

| Dataset Size | Index Type | Query Pattern | Expected Latency |
|-------------|-----------|---------------|-----------------|
| 100K rows | text_pattern_ops B-tree | `LIKE 'PREFIX%'` | <1ms |
| 1M rows | text_pattern_ops B-tree | `LIKE 'PREFIX%'` | 1-2ms |
| 10M rows | text_pattern_ops B-tree | `LIKE 'PREFIX%'` | 2-5ms |
| 1M rows | pg_trgm GIN | `%INFIX%` or similarity | 10-30ms |

The `text_pattern_ops` index is optimal for prefix-only matching. If fuzzy matching is needed later, `pg_trgm` can be added as a supplementary index.

**Note**: With the new `addresses` table, the prefix search index moves to `addresses.normalized_address` instead of `geocoder_cache.normalized_address`. This is architecturally cleaner since addresses is the canonical store and avoids duplicate results from multiple providers.

### 5. Georgia Bounding Box Coordinates

**Decision**: Hardcode Georgia's approximate bounding box as constants.

**Values** (WGS84):

| Parameter | Value | Description |
|-----------|-------|-------------|
| `GA_MIN_LAT` | 30.355 | Southern border (Florida line) |
| `GA_MAX_LAT` | 35.001 | Northern border (Tennessee/North Carolina line) |
| `GA_MIN_LNG` | -85.606 | Western border (Alabama line) |
| `GA_MAX_LNG` | -80.840 | Eastern border (Atlantic coast/South Carolina line) |

These values include a small buffer beyond the strict border to accommodate GPS inaccuracy near state boundaries.

**Rationale**: A bounding box check is computationally trivial and catches obvious out-of-area requests before the more expensive spatial query. It is not a substitute for precise boundary containment — addresses at the very edge of the bounding box might still be outside Georgia. This is acceptable because the geocoding provider will return Georgia-specific results, and the point-lookup will return empty results for points outside loaded boundaries.

### 6. Accuracy Radius: Meter-to-Degree Conversion

**Decision**: Use a latitude-dependent approximation for converting GPS accuracy (meters) to degrees.

**Formula**:

```python
import math

def meters_to_degrees(meters: float, latitude: float) -> float:
    """Convert meters to approximate degrees at a given latitude."""
    # 1 degree of latitude ≈ 111,320 meters (constant)
    # 1 degree of longitude ≈ 111,320 * cos(latitude) meters
    lat_deg = meters / 111_320
    lng_deg = meters / (111_320 * math.cos(math.radians(latitude)))
    # Return the larger of the two for conservative radius
    return max(lat_deg, lng_deg)
```

**Rationale**: At Georgia's latitude (30-35°N), the approximation error is <3%. For GPS accuracy radii (<1km), this translates to <30m error — well within the uncertainty of consumer-grade GPS. PostGIS `ST_DWithin` with geography type would be more accurate but requires casting geometry to geography, which is slower for point-in-polygon on MULTIPOLYGON boundaries.

**Alternative considered**: Use `ST_Buffer` to create a circle geometry and then `ST_Intersects`. This is more geometrically correct but slower than `ST_DWithin`.

### 7. Provider Error Differentiation

**Decision**: Modify `CensusGeocoder.geocode()` to raise `GeocodingProviderError` on transport/HTTP errors instead of returning `None`. Return `None` only for "no match" (provider responded successfully, but `addressMatches` is empty).

**Current behavior**: `CensusGeocoder.geocode()` returns `None` for both no-match and provider errors (timeout, HTTP error, parse failure). The new endpoint needs to return different HTTP status codes (404 vs 502).

**Approach**: The geocoder library communicates the distinction clearly:
1. `GeocodingResult` returned → success (200)
2. `None` returned → no match, provider responded but found nothing (404)
3. `GeocodingProviderError` raised → transport/HTTP/timeout error (502)

Batch processing must be updated to catch `GeocodingProviderError` (treat as retriable failure, same as current `None` handling for errors).

**Rationale**: This is the cleanest approach — the library communicates the distinction clearly, and both batch and single-address consumers can handle it appropriately.

### 8. Canonical Address Entity Design

**Decision**: Introduce a dedicated `addresses` table as the canonical address store.

**Context**: The user requires that address data be maintained independently of other entities (voters, geocoder results). This ensures address identity persists even when referencing data changes or is removed.

**Architecture**:

```
addresses (canonical store)
├── geocoder_cache (FK: address_id) — provider-specific geocoding results
├── voters (FK: residence_address_id) — voter registration records
└── (future entities can reference addresses via FK)
```

**Key design decisions**:

1. **Content**: Both parsed component fields (street_number, street_name, street_type, pre_direction, post_direction, apt_unit, city, state, zipcode) AND a normalized freeform string (`normalized_address`). Components enable structured queries and display; the normalized string enables cache lookups and deduplication.

2. **Uniqueness**: UNIQUE constraint on `normalized_address`. The normalized form (uppercased, USPS-abbreviated, whitespace-collapsed) ensures "100 Peachtree Street NW" and "100 PEACHTREE ST NW" resolve to the same row.

3. **Creation policy**: Address rows are created on successful geocode only (from the API endpoint). Failed/unmatchable addresses never get a row. Voter import also creates/links address rows.

4. **Voter relationship**: Dual-write. Voter table gets `residence_address_id` FK (nullable) but keeps all inline address fields permanently — they represent government-sourced truth from the GA SoS voter file.

5. **Geocoder cache relationship**: `geocoder_cache` gets `address_id` FK (nullable for backward compatibility). One address can have results from multiple providers.

### 9. Address Entity Migration Strategy

**Decision**: Phased migration with nullable FKs for backward compatibility.

**Phase 1 — Schema changes** (Alembic migration):
1. Create `addresses` table with all columns and UNIQUE constraint on `normalized_address`
2. Add `address_id` FK column (nullable) to `geocoder_cache`
3. Add `residence_address_id` FK column (nullable) to `voters`
4. Add prefix search index on `addresses.normalized_address` (text_pattern_ops)
5. Add FK indexes on `geocoder_cache.address_id` and `voters.residence_address_id`

**Phase 2 — Backfill** (data migration, can run separately):
1. For each unique `normalized_address` in `geocoder_cache`: parse components from the normalized string, create an `addresses` row, update `geocoder_cache.address_id`
2. For each voter with residence address fields: reconstruct normalized address from components, find-or-create `addresses` row, set `voters.residence_address_id`

**Rationale**: Nullable FKs allow the schema migration to be non-destructive and backward-compatible. Existing batch geocoding continues to work. The backfill can run as a background task or CLI command without blocking deployment.

**Alternatives considered**:
- Non-nullable FKs with immediate backfill in migration: Too risky for 7M+ voter records in a single migration
- Skip voter backfill entirely: Would leave voters disconnected from the address entity, defeating the purpose
